# -*- coding: utf-8 -*-
"""RAG Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11oMhHYvU8xOy23oKeFybke3qu6TKXYEB

##Installing dependencies and imports
"""

!pip install -q gradio==3.50.2 pypdf2==3.0.1 python-docx==0.8.11 faiss-cpu==1.7.4 \
    transformers==4.30.2 torch==2.0.1 sentence-transformers==2.2.2 \
    langchain langchain-community ctransformers python-jose

# Import required libraries
import os
import gradio as gr
from typing import List, Dict
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
import faiss
import PyPDF2
from docx import Document
import pickle
from langchain_community.llms import CTransformers
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import faiss
import numpy as np

!pip install -q redis python-jose

"""##Setting up Spark"""

import json
import os

from google.colab import files

if 'spark_nlp_for_healthcare_spark_ocr_9813.json' not in os.listdir():
  license_keys = files.upload()
  os.rename(list(license_keys.keys())[0], 'spark_nlp_for_healthcare_spark_ocr_9813.json')

with open('spark_nlp_for_healthcare_spark_ocr_9813.json') as f:
    license_keys = json.load(f)

# Defining license key-value pairs as local variables
locals().update(license_keys)
os.environ.update(license_keys)

license_keys.keys()
license_keys['JSL_VERSION']
license_keys['PUBLIC_VERSION']

# Installing pyspark and spark-nlp
! pip install --upgrade -q pyspark==3.4.0 spark-nlp==$PUBLIC_VERSION

# Installing Spark NLP Healthcare
! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET

# Installing Spark NLP Display Library for visualization
! pip install -q spark-nlp-display

#Setting up spark
import sparknlp
import sparknlp_jsl

from sparknlp.base import *
from sparknlp.annotator import *
from sparknlp_jsl.annotator import *
from sparknlp_jsl.pipeline_tracer import PipelineTracer
from sparknlp_jsl.pipeline_output_parser import PipelineOutputParser

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml import Pipeline,PipelineModel

import json
import pandas as pd
pd.set_option('display.max_colwidth', 200)

from pyspark.sql.functions import col, explode, lit

from tqdm import tqdm
from collections import Counter

import warnings
warnings.filterwarnings('ignore')

params = {"spark.driver.memory":"10G",
          "spark.kryoserializer.buffer.max":"2000M",
          "spark.driver.maxResultSize":"2000M"}

print("Spark NLP Version :", sparknlp.version())
print("Spark NLP_JSL Version :", sparknlp_jsl.version())

spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)

spark

! pip install pymupdf python-docx pyspark

"""##LLM"""

! pip install transformers
! pip install torch
! pip install keras

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Encode the input text


def chat_with_claude(prompt):
  inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
  # tokenizer.pad_token = tokenizer.eos_token
  inputs["pad_token_id"] = tokenizer.pad_token_id
  # Generate a response
  # outputs = model.generate(inputs['input_ids'], max_length=500, num_return_sequences=1)
  outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], pad_token_id=inputs['pad_token_id'], max_length=200)

  # Decode the output
  # print(outputs[0])
  response = tokenizer.decode(outputs[0], skip_special_tokens=True)
  print("printing response")
  print(response)
  return response

! export HF_TOKEN=********** #Insert huggingface access token here

from transformers import BloomTokenizerFast, BloomForCausalLM

# Load BLOOM model and tokenizer
model_name = "bigscience/bloom-1b7"
tokenizer = BloomTokenizerFast.from_pretrained(model_name)
model = BloomForCausalLM.from_pretrained(model_name)

# Tokenize input text
def test(prompt):
# input_text = "What is the capital of France?"
  inputs = tokenizer(prompt, return_tensors="pt")

  # Generate the response
  outputs = model.generate(inputs['input_ids'], max_length=200)

  # Decode and print the response
  response = tokenizer.decode(outputs[0], skip_special_tokens=True)
  print(response)
  return response

def create_prompt(context, question):
    return f"""Use the following context to answer the question. make the answer concise to 10 words or less than 10 words
    If you don't find the answer in the context, say "I don't have enough information to answer that."

    Context: {context}

    Question: {question}"""

"""##Vector DB"""

def search_similar(faiss_index, question_embedding, file_text, k):
    print("calling search similar function")
    # return search_faiss(question_embedding, file_text ,k=3)
    query_embedding = np.array(question_embedding).reshape(1, -1).astype('float32')
    distances, indices = faiss_index.search(query_embedding, k)
    results = [(file_text[idx], distances[0][i]) for i, idx in enumerate(indices[0])]
    return results

def create_faiss_index(df):
    dimension = len(df.iloc[0]['embeddings'])  # Assuming all embeddings are of the same length
    faiss_index = faiss.IndexFlatL2(dimension)
    print("faiss index created")
    return faiss_index

"""##Doc processing"""

from pyspark.sql.types import StructType, StructField, StringType
def process_file(file_text):
    schema = StructType([
        StructField("text", StringType(), True)
    ])
    result_df = spark.createDataFrame(file_text, schema)
    # import sparknlp


    # # Define the pipeline
    document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
    sentence_detector = SentenceDetector().setInputCols("document").setOutputCol("sentence")
    embeddings = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en', 'clinical/models').setInputCols(["sentence"]).setOutputCol("sentence_embeddings")

    # # Combine into a pipeline
    pipeline = Pipeline(stages=[document_assembler, sentence_detector, embeddings])

    # # Example text
    data = spark.createDataFrame(file_text, schema)

    # # Apply the pipeline
    model = pipeline.fit(data)
    result = model.transform(data).cache()
    resultWithSize = result.selectExpr(
        "sentence.result as result",
        "sentence_embeddings.embeddings as embeddings"
    )
    res2 = resultWithSize.selectExpr(
        "arrays_zip(result, embeddings) as result_embeddings"
    )
    final = res2.select(explode("result_embeddings").alias("name_age_pair"))
    final_max = final.selectExpr(
        "name_age_pair.result as result",
        "name_age_pair.embeddings as embeddings"
    ).toPandas()
    return final_max

df = process_file(file_text=[["this is sentence one. this is sentence two .this is sentence three."]])
df

"""##Caching and Auth"""

# Simple in-memory cache for Colab
cache = {}

# To get a value from the cache
cache_get = cache.get

# To set a value in the cache
cache_set = lambda key, value: cache.update({key: value})

# To clear the cache
cache_clear = cache.clear

import jwt
import datetime

# Secret key for encoding/decoding tokens (use a secure key in production)
SECRET_KEY = "your-secret-key"

# Check user credentials
check_credentials = lambda username, password: username == "username" and password == "password"

# Create a JWT token
create_token = lambda username: jwt.encode(
    {"username": username, "exp": datetime.datetime.utcnow() + datetime.timedelta(hours=1)},
    SECRET_KEY,
    algorithm="HS256"
)

# Verify a JWT token
def verify_token(token: str) -> bool:
    try:
        jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        return True
    except:
        return False

"""##UI Integration"""

import gradio as gr
from docx import Document
import fitz
faiss_index = ''
texts = []
# Function to handle file upload
def handle_file_upload(file):
    if file is None:
        return "Please upload a file first."

    try:
        # Read file content based on file type
        if file.name.endswith('.pdf'):
            print("line 13 detected pdf")
            # file_text = fitz.open(file.name).get_text()
            pdf_doc = fitz.open(file.name)
            file_text = ""
            for page in pdf_doc:
                print("line 18 looping over pages")
                file_text += page.get_text()
            pdf_doc.close()
        elif file.name.endswith('.docx'):
            doc = Document(file.name)
            file_text = "\n".join([p.text for p in doc.paragraphs])
        else:
            return "Unsupported file format. Please upload PDF or DOCX files."

        # Process file content
        # print(type(file_text))
        if isinstance(file_text, str):
          data = [[file_text]]
        result = process_file(data)
        print(f"length of embeddings or texts {len(result['embeddings'].tolist())}")
        print("processed file text")

        global faiss_index
        faiss_index = create_faiss_index(result)

        faiss_index.add(np.array(result['embeddings'].tolist()).astype('float32'))
        print(f"type of {type(faiss_index)} ")
        print("embeddings recieved")

        # Add texts to global storage
        # new_texts = get_texts(result)
        # print(new_texts)
        # texts.extend(new_texts)
        global texts
        texts = result['result'].to_list()
        print("Text to global storage done")
        return {"status": 200, "body": "File processed successfully!"}

    except Exception as e:
        return f"Error processing file: {str(e)}"

# Function to handle question answering
def handle_question(question):
    if not question:
        return "Please enter a question."

    # Check cache
    # cache_key = hash_key(question)
    # cached_answer = cache.get(cache_key)
    # if cached_answer:
    #     return f"{cached_answer} (Cached)"

    try:
        # Get question embedding
        if isinstance(question, str):
          data = [[question]]
        question_result = process_file(data)
        question_embedding = question_result['embeddings'].to_list()
        print("question embeddings generated")
        # Search similar documents
        global faiss_index
        global texts
        indices = search_similar(faiss_index, question_embedding, texts, k=10)
        print(indices)
        relevant_chunks = [i[0] for i in indices]
        print(relevant_chunks)

        print("relevant_chunks")

        # Create prompt and get answer
        context = "\n".join(relevant_chunks)
        prompt = create_prompt(context, question)
        # print(prompt)
        answer = chat_with_claude(prompt)

        # Cache answer
        # cache[cache_key] = answer

        return answer

    except Exception as e:
        return f"Error generating answer: {str(e)}"

# Create Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# Medical Document QA System")

    with gr.Row():
        file_input = gr.File(
            label="Upload PDF or DOCX file",
            file_types=[".pdf", ".docx"]
        )
        process_button = gr.Button("Process File")
        output_text = gr.Textbox(label="Processing Status")

    with gr.Row():
        question_input = gr.Textbox(label="Ask a question about the documents")
        submit_button = gr.Button("Get Answer")

    answer_output = gr.Textbox(label="Answer")

    process_button.click(
        fn=handle_file_upload,
        inputs=[file_input],
        outputs=output_text,
        api_name="process_file",
        show_progress=True
    )

    submit_button.click(
        fn=handle_question,
        inputs=[question_input],
        outputs=answer_output
    )

# Launch the interface
demo.queue(concurrency_count=1, max_size=20).launch(
    debug=True,
    share=True,
    max_threads=1  # Controls concurrent processing
)

