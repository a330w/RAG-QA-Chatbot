{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb9xPoaeWxli"
      },
      "source": [
        "##Installing dependencies and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaR2QqTpWfUz",
        "outputId": "888d14fd-c74b-4bea-f841-bcb7aa49d51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m5.1/5.6 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.3.0 requires websockets<15.0dev,>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio==3.50.2 pypdf2==3.0.1 python-docx==0.8.11 faiss-cpu==1.7.4 \\\n",
        "    transformers==4.30.2 torch==2.0.1 sentence-transformers==2.2.2 \\\n",
        "    langchain langchain-community ctransformers python-jose\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import gradio as gr\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import faiss\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import pickle\n",
        "from langchain_community.llms import CTransformers\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGYYAnQEWxGY",
        "outputId": "6c87dcbf-f9c0-46ce-fd17-ebc5e5baf6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/261.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q redis python-jose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-6woAFEW-aC"
      },
      "source": [
        "##Setting up Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "IvSdG0vlk_3a",
        "outputId": "884c2fa8-f2fc-464b-d06b-03dd609d840c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ea3614c-0a24-413a-8595-eaae0ff6500f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ea3614c-0a24-413a-8595-eaae0ff6500f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving spark_nlp_for_healthcare_spark_ocr_9813.json to spark_nlp_for_healthcare_spark_ocr_9813.json\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.6/626.6 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.9/538.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if 'spark_nlp_for_healthcare_spark_ocr_9813.json' not in os.listdir():\n",
        "  license_keys = files.upload()\n",
        "  os.rename(list(license_keys.keys())[0], 'spark_nlp_for_healthcare_spark_ocr_9813.json')\n",
        "\n",
        "with open('spark_nlp_for_healthcare_spark_ocr_9813.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "os.environ.update(license_keys)\n",
        "\n",
        "license_keys.keys()\n",
        "license_keys['JSL_VERSION']\n",
        "license_keys['PUBLIC_VERSION']\n",
        "\n",
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.4.0 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "# Installing Spark NLP Display Library for visualization\n",
        "! pip install -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "OSj4sl82WxDs",
        "outputId": "23fcf5f6-4f27-4757-9295-af3f6490b52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP Version : 5.5.1\n",
            "Spark NLP_JSL Version : 5.5.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ac0806e0ee0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://458b13213746:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP Licensed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#Setting up spark\n",
        "import sparknlp\n",
        "import sparknlp_jsl\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp_jsl.pipeline_tracer import PipelineTracer\n",
        "from sparknlp_jsl.pipeline_output_parser import PipelineOutputParser\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "from pyspark.sql.functions import col, explode, lit\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "params = {\"spark.driver.memory\":\"10G\",\n",
        "          \"spark.kryoserializer.buffer.max\":\"2000M\",\n",
        "          \"spark.driver.maxResultSize\":\"2000M\"}\n",
        "\n",
        "print(\"Spark NLP Version :\", sparknlp.version())\n",
        "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXDvbDQAWxAY",
        "outputId": "84f0ac4f-f431-434e-9471-7f826da62add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (0.8.11)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.1\n"
          ]
        }
      ],
      "source": [
        "#Installing pdf and doc readers.\n",
        "! pip install pymupdf python-docx pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9hzfvHYXzka"
      },
      "source": [
        "##LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxPP2vDpfS6O",
        "outputId": "995f81c9-72c4-43cd-f132-95e2db4c62e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install torch\n",
        "! pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmp9f_QAkEkZ"
      },
      "outputs": [],
      "source": [
        "#Setting up the LLM\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Encode the input text\n",
        "\n",
        "\n",
        "def chat_with_claude(prompt):\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "  # tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "  # Generate a response\n",
        "  # outputs = model.generate(inputs['input_ids'], max_length=500, num_return_sequences=1)\n",
        "  outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], pad_token_id=inputs['pad_token_id'], max_length=200)\n",
        "\n",
        "  # Decode the output\n",
        "  # print(outputs[0])\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  print(\"printing response\")\n",
        "  print(response)\n",
        "  return response\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Huggngface token\n",
        "! export HF_TOKEN=******** #Insert huggingface access token here"
      ],
      "metadata": {
        "id": "NwkbDeq9XY-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agwTWSp4j7uA"
      },
      "outputs": [],
      "source": [
        "#Creating the prompt\n",
        "def create_prompt(context, question):\n",
        "    return f\"\"\"Use the following context to answer the question. make the answer concise to 10 words or less than 10 words\n",
        "    If you don't find the answer in the context, say \"I don't have enough information to answer that.\"\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {question}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector DB"
      ],
      "metadata": {
        "id": "mgUqzqPCj3YM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jNVnZr0i8ro"
      },
      "outputs": [],
      "source": [
        "#Similarity Search\n",
        "def search_similar(faiss_index, question_embedding, file_text, k):\n",
        "    print(\"calling search similar function\")\n",
        "    # return search_faiss(question_embedding, file_text ,k=3)\n",
        "    query_embedding = np.array(question_embedding).reshape(1, -1).astype('float32')\n",
        "    distances, indices = faiss_index.search(query_embedding, k)\n",
        "    results = [(file_text[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtGymkUhhVFU"
      },
      "outputs": [],
      "source": [
        "#FAISS Index\n",
        "def create_faiss_index(df):\n",
        "    dimension = len(df.iloc[0]['embeddings'])  # Assuming all embeddings are of the same length\n",
        "    faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    print(\"faiss index created\")\n",
        "    return faiss_index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Doc processing"
      ],
      "metadata": {
        "id": "rmIEcTgVj6qK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gKP-dJkbIxJ"
      },
      "outputs": [],
      "source": [
        "#Doc processing using Spark Pipeline\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "def process_file(file_text):\n",
        "    schema = StructType([\n",
        "        StructField(\"text\", StringType(), True)\n",
        "    ])\n",
        "    result_df = spark.createDataFrame(file_text, schema)\n",
        "    # import sparknlp\n",
        "\n",
        "\n",
        "    # # Define the pipeline\n",
        "    document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "    sentence_detector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
        "    embeddings = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en', 'clinical/models').setInputCols([\"sentence\"]).setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "    # # Combine into a pipeline\n",
        "    pipeline = Pipeline(stages=[document_assembler, sentence_detector, embeddings])\n",
        "\n",
        "    # # Example text\n",
        "    data = spark.createDataFrame(file_text, schema)\n",
        "\n",
        "    # # Apply the pipeline\n",
        "    model = pipeline.fit(data)\n",
        "    result = model.transform(data).cache()\n",
        "    resultWithSize = result.selectExpr(\n",
        "        \"sentence.result as result\",\n",
        "        \"sentence_embeddings.embeddings as embeddings\"\n",
        "    )\n",
        "    res2 = resultWithSize.selectExpr(\n",
        "        \"arrays_zip(result, embeddings) as result_embeddings\"\n",
        "    )\n",
        "    final = res2.select(explode(\"result_embeddings\").alias(\"name_age_pair\"))\n",
        "    final_max = final.selectExpr(\n",
        "        \"name_age_pair.result as result\",\n",
        "        \"name_age_pair.embeddings as embeddings\"\n",
        "    ).toPandas()\n",
        "    return final_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "v7CP1ywOGhwh",
        "outputId": "0545b112-da3d-4027-9889-84563765625c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sbiobert_base_cased_mli download started this may take some time.\n",
            "Approximate size to download 384.3 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    result  \\\n",
              "0    this is sentence one.   \n",
              "1   this is sentence two .   \n",
              "2  this is sentence three.   \n",
              "\n",
              "                                                                                                                                                                                                embeddings  \n",
              "0  [0.35628798604011536, -0.48671117424964905, 0.0836561992764473, 0.22219394147396088, 1.161763310432434, 0.2215859740972519, -0.18069948256015778, 0.354088693857193, -0.08967858552932739, 0.6732552...  \n",
              "1  [0.14464564621448517, -0.25935453176498413, -0.6000803112983704, 0.629290759563446, 1.0368129014968872, 0.11163471639156342, -0.22896036505699158, -0.014659400098025799, 1.1230627298355103, 0.1673...  \n",
              "2  [0.7257365584373474, -0.20896494388580322, -0.7927187085151672, 0.28782516717910767, 1.0485575199127197, -0.04332759603857994, -0.04275362938642502, 0.7431215047836304, -0.30505460500717163, 0.648...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad287b40-aff4-4933-ad71-cceeed4a8133\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>result</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this is sentence one.</td>\n",
              "      <td>[0.35628798604011536, -0.48671117424964905, 0.0836561992764473, 0.22219394147396088, 1.161763310432434, 0.2215859740972519, -0.18069948256015778, 0.354088693857193, -0.08967858552932739, 0.6732552...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this is sentence two .</td>\n",
              "      <td>[0.14464564621448517, -0.25935453176498413, -0.6000803112983704, 0.629290759563446, 1.0368129014968872, 0.11163471639156342, -0.22896036505699158, -0.014659400098025799, 1.1230627298355103, 0.1673...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is sentence three.</td>\n",
              "      <td>[0.7257365584373474, -0.20896494388580322, -0.7927187085151672, 0.28782516717910767, 1.0485575199127197, -0.04332759603857994, -0.04275362938642502, 0.7431215047836304, -0.30505460500717163, 0.648...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad287b40-aff4-4933-ad71-cceeed4a8133')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad287b40-aff4-4933-ad71-cceeed4a8133 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad287b40-aff4-4933-ad71-cceeed4a8133');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-32c1515c-5838-44f2-8521-92010ec240eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-32c1515c-5838-44f2-8521-92010ec240eb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-32c1515c-5838-44f2-8521-92010ec240eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_92bae4c6-ebfe-47ad-963a-2fd6c090a7ec\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_92bae4c6-ebfe-47ad-963a-2fd6c090a7ec button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"result\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"this is sentence one.\",\n          \"this is sentence two .\",\n          \"this is sentence three.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#Test df to check chunking and embedding.\n",
        "df = process_file(file_text=[[\"this is sentence one. this is sentence two .this is sentence three.\"]])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7SANxtn1Cbq"
      },
      "source": [
        "##Caching and Auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smuDHqMb1B4r"
      },
      "outputs": [],
      "source": [
        "#Caching\n",
        "# Simple in-memory cache for Colab\n",
        "cache = {}\n",
        "\n",
        "# To get a value from the cache\n",
        "cache_get = cache.get\n",
        "\n",
        "# To set a value in the cache\n",
        "cache_set = lambda key, value: cache.update({key: value})\n",
        "\n",
        "# To clear the cache\n",
        "cache_clear = cache.clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZiXAYI11GMH"
      },
      "outputs": [],
      "source": [
        "#Authenticaton\n",
        "import jwt\n",
        "import datetime\n",
        "\n",
        "# Secret key for encoding/decoding tokens (use a secure key in production)\n",
        "SECRET_KEY = \"your-secret-key\"\n",
        "\n",
        "# Check user credentials\n",
        "check_credentials = lambda username, password: username == \"username\" and password == \"password\"\n",
        "\n",
        "# Create a JWT token\n",
        "create_token = lambda username: jwt.encode(\n",
        "    {\"username\": username, \"exp\": datetime.datetime.utcnow() + datetime.timedelta(hours=1)},\n",
        "    SECRET_KEY,\n",
        "    algorithm=\"HS256\"\n",
        ")\n",
        "\n",
        "# Verify a JWT token\n",
        "def verify_token(token: str) -> bool:\n",
        "    try:\n",
        "        jwt.decode(token, SECRET_KEY, algorithms=[\"HS256\"])\n",
        "        return True\n",
        "    except:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UI Integration"
      ],
      "metadata": {
        "id": "_zSwjfAzj_ip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mYwGJ1Q5bJEU",
        "outputId": "2c525aa9-7cd1-4d59-95ac-2098f1c34078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://df04c2b959059e4785.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://df04c2b959059e4785.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "line 13 detected pdf\n",
            "line 18 looping over pages\n",
            "line 18 looping over pages\n",
            "line 18 looping over pages\n",
            "sbiobert_base_cased_mli download started this may take some time.\n",
            "Approximate size to download 384.3 MB\n",
            "[OK!]\n",
            "length of embeddings or texts 36\n",
            "processed file text\n",
            "faiss index created\n",
            "type of <class 'faiss.swigfaiss_avx2.IndexFlatL2'> \n",
            "embeddings recieved\n",
            "Text to global storage done\n",
            "sbiobert_base_cased_mli download started this may take some time.\n",
            "Approximate size to download 384.3 MB\n",
            "[OK!]\n",
            "question embeddings generated\n",
            "calling search similar function\n",
            "[('Mental health exists on a continuum, or range: \\nfrom positive, healthy functioning at one end \\nthrough to severe symptoms of mental health \\nconditions at the other.', 162.40167), ('Like physical health, mental health is not fixed.', 167.69418), ('•\\t\\nWhat will you use next time you speak about mental health conditions?', 177.55908), ('UNDERSTANDING MENTAL HEALTH\\nWHAT IS MENTAL HEALTH?', 189.18271), ('If you are concerned about yourself or a colleague, \\nprofessional assistance is recommended.', 219.62509), ('Good mental health and well-being is not an on-off \\nexperience.', 224.96329), ('Risk factors and protective factors influence mental health and can \\nnudge people back and forth along the continuum.', 232.4158), ('Our mental health directly influences how we think, feel and act: it also affects our physical health.', 252.45282), ('This could \\nlead to us developing a mental health condition such \\nas anxiety, depression, substance misuse.', 253.93546), ('They can be \\nindividual or related to family, work or other life circumstances.', 261.2537)]\n",
            "['Mental health exists on a continuum, or range: \\nfrom positive, healthy functioning at one end \\nthrough to severe symptoms of mental health \\nconditions at the other.', 'Like physical health, mental health is not fixed.', '•\\t\\nWhat will you use next time you speak about mental health conditions?', 'UNDERSTANDING MENTAL HEALTH\\nWHAT IS MENTAL HEALTH?', 'If you are concerned about yourself or a colleague, \\nprofessional assistance is recommended.', 'Good mental health and well-being is not an on-off \\nexperience.', 'Risk factors and protective factors influence mental health and can \\nnudge people back and forth along the continuum.', 'Our mental health directly influences how we think, feel and act: it also affects our physical health.', 'This could \\nlead to us developing a mental health condition such \\nas anxiety, depression, substance misuse.', 'They can be \\nindividual or related to family, work or other life circumstances.']\n",
            "relevant_chunks\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://df04c2b959059e4785.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#Using gradio for UI integration and connection with the models\n",
        "import gradio as gr\n",
        "from docx import Document\n",
        "import fitz\n",
        "faiss_index = ''\n",
        "texts = []\n",
        "# Function to handle file upload\n",
        "def handle_file_upload(file):\n",
        "    if file is None:\n",
        "        return \"Please upload a file first.\"\n",
        "\n",
        "    try:\n",
        "        # Read file content based on file type\n",
        "        if file.name.endswith('.pdf'):\n",
        "            print(\"line 13 detected pdf\")\n",
        "            # file_text = fitz.open(file.name).get_text()\n",
        "            pdf_doc = fitz.open(file.name)\n",
        "            file_text = \"\"\n",
        "            for page in pdf_doc:\n",
        "                print(\"line 18 looping over pages\")\n",
        "                file_text += page.get_text()\n",
        "            pdf_doc.close()\n",
        "        elif file.name.endswith('.docx'):\n",
        "            doc = Document(file.name)\n",
        "            file_text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "        else:\n",
        "            return \"Unsupported file format. Please upload PDF or DOCX files.\"\n",
        "\n",
        "        # Process file content\n",
        "        # print(type(file_text))\n",
        "        if isinstance(file_text, str):\n",
        "          data = [[file_text]]\n",
        "        result = process_file(data)\n",
        "        print(f\"length of embeddings or texts {len(result['embeddings'].tolist())}\")\n",
        "        print(\"processed file text\")\n",
        "\n",
        "        global faiss_index\n",
        "        faiss_index = create_faiss_index(result)\n",
        "\n",
        "        faiss_index.add(np.array(result['embeddings'].tolist()).astype('float32'))\n",
        "        print(f\"type of {type(faiss_index)} \")\n",
        "        print(\"embeddings recieved\")\n",
        "\n",
        "        # Add texts to global storage\n",
        "        # new_texts = get_texts(result)\n",
        "        # print(new_texts)\n",
        "        # texts.extend(new_texts)\n",
        "        global texts\n",
        "        texts = result['result'].to_list()\n",
        "        print(\"Text to global storage done\")\n",
        "        return {\"status\": 200, \"body\": \"File processed successfully!\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing file: {str(e)}\"\n",
        "\n",
        "# Function to handle question answering\n",
        "def handle_question(question):\n",
        "    if not question:\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    # Check cache\n",
        "    # cache_key = hash_key(question)\n",
        "    # cached_answer = cache.get(cache_key)\n",
        "    # if cached_answer:\n",
        "    #     return f\"{cached_answer} (Cached)\"\n",
        "\n",
        "    try:\n",
        "        # Get question embedding\n",
        "        if isinstance(question, str):\n",
        "          data = [[question]]\n",
        "        question_result = process_file(data)\n",
        "        question_embedding = question_result['embeddings'].to_list()\n",
        "        print(\"question embeddings generated\")\n",
        "        # Search similar documents\n",
        "        global faiss_index\n",
        "        global texts\n",
        "        indices = search_similar(faiss_index, question_embedding, texts, k=10)\n",
        "        print(indices)\n",
        "        relevant_chunks = [i[0] for i in indices]\n",
        "        print(relevant_chunks)\n",
        "\n",
        "        print(\"relevant_chunks\")\n",
        "\n",
        "        # Create prompt and get answer\n",
        "        context = \"\\n\".join(relevant_chunks)\n",
        "        prompt = create_prompt(context, question)\n",
        "        # print(prompt)\n",
        "        answer = chat_with_claude(prompt)\n",
        "\n",
        "        # Cache answer\n",
        "        # cache[cache_key] = answer\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Medical Document QA System\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(\n",
        "            label=\"Upload PDF or DOCX file\",\n",
        "            file_types=[\".pdf\", \".docx\"]\n",
        "        )\n",
        "        process_button = gr.Button(\"Process File\")\n",
        "        output_text = gr.Textbox(label=\"Processing Status\")\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"Ask a question about the documents\")\n",
        "        submit_button = gr.Button(\"Get Answer\")\n",
        "\n",
        "    answer_output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=handle_file_upload,\n",
        "        inputs=[file_input],\n",
        "        outputs=output_text,\n",
        "        api_name=\"process_file\",\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=handle_question,\n",
        "        inputs=[question_input],\n",
        "        outputs=answer_output\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.queue(concurrency_count=1, max_size=20).launch(\n",
        "    debug=True,\n",
        "    share=True,\n",
        "    max_threads=1  # Controls concurrent processing\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFlCQBIBdhYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}